{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B&W Colorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an experiment to see how well a model can re-colorize historical B&W images and videos. I have seen some advanced GAN examples and wondered if a simple Unet based on a standard resnet could be enough to give ok-ish results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import fastai\n",
    "import fastai.vision\n",
    "from fastai.vision.models import DynamicUnet\n",
    "from fastai.vision.models import resnet34\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/jupyter/.fastai/data/imagenette2-320\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "_train_ds = datasets.ImageFolder(root=data_path + '/train', transform=_data_transform)\n",
    "_val_ds   = datasets.ImageFolder(root=data_path + '/val', transform=_data_transform)\n",
    "\n",
    "# This should be \"None\" if the entire dataset should be used\n",
    "subset_size = 1#batch_size # None\n",
    "_train_ds = torch.utils.data.Subset(_train_ds, list(range(0, subset_size if subset_size is not None else len(_train_ds))))\n",
    "_val_ds = torch.utils.data.Subset(_train_ds, list(range(0, subset_size if subset_size is not None else len(_train_ds))))\n",
    "\n",
    "training_dataset_loader   = torch.utils.data.DataLoader(_train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "validation_dataset_loader = torch.utils.data.DataLoader(_val_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "We will use a simple pretrained resnet34 as a base for a unet by chopping off the head and using the fastai DynamicUnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_resnet = resnet34(pretrained=True)\n",
    "_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "_resnet = nn.Sequential(*list(_resnet.children())[:-2])\n",
    "\n",
    "model = DynamicUnet(_resnet, img_size=(224,224), n_classes=3); model # We want three-channel images as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(training_dataset_loader))[0][0,:,:,:]\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(x):\n",
    "    return transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "            std=[1/0.229, 1/0.224, 1/0.255]\n",
    "        ),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ])(x)\n",
    "\n",
    "grayscale_img = grayscale(img)\n",
    "plt.imshow(grayscale_img.repeat(3,1,1).permute(1,2,0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(grayscale_img.unsqueeze(0))\n",
    "plt.imshow(output.squeeze().permute(1,2,0).detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def fit(model=model, training_dataset_loader=training_dataset_loader, validation_dataset_loader=validation_dataset_loader, epochs=10, max_learning_rate=1e-6):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=max_learning_rate)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    model = model.to(device)\n",
    "    for e in range(epochs):\n",
    "        training_loss = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        for i, batch in enumerate(tqdm(training_dataset_loader)):\n",
    "            color_imgs = batch[0].detach().to(device)\n",
    "            grayscale_imgs = torch.stack([grayscale(img) for img in batch[0]]).to(device)\n",
    "            outputs = model(grayscale_imgs)\n",
    "            loss = criterion(outputs, color_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            training_loss += loss.detach().cpu().item()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Validate\n",
    "        for i, batch in enumerate(tqdm(validation_dataset_loader)):\n",
    "            with torch.no_grad():\n",
    "                color_imgs = batch[0].detach().to(device)\n",
    "                grayscale_imgs = torch.stack([grayscale(img) for img in batch[0]]).to(device)\n",
    "                outputs = model(grayscale_imgs)\n",
    "                loss = criterion(outputs, color_imgs)\n",
    "                validation_loss += loss.detach().cpu().item()\n",
    "            \n",
    "        training_loss /= len(training_dataset_loader)\n",
    "        validation_loss /= len(validation_dataset_loader)\n",
    "            \n",
    "        print(f\"Epoch {e+1} finished. \\t Training loss: {training_loss} \\t validation loss: {validation_loss}\")\n",
    "        # torch.save(model, f'./models/colorizer_model_{datetime.now()}-lr_{max_learning_rate}_e_{epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs=50, max_learning_rate=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test the model on an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = next(iter(training_dataset_loader))[0][0,:,:,:]\n",
    "output = model(grayscale(img).unsqueeze(0).cuda()).detach().cpu().squeeze()\n",
    "\n",
    "#print(img.mean(), img.std())\n",
    "#print(output.mean(), output.std())\n",
    "\n",
    "output -= output.mean()\n",
    "output /= output.std() + 1e-3\n",
    "output *= .15\n",
    "output = np.clip(output, 0, 1)\n",
    "def reset(x):\n",
    "    return transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "            std=[1/0.229, 1/0.224, 1/0.255]\n",
    "        ),\n",
    "    ])(x)\n",
    "print(output)\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(img.squeeze().permute(1,2,0))\n",
    "axarr[1].imshow(reset(output).squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(\"./test_img.jpg\").resize((224,224))\n",
    "test_img_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])(test_img)\n",
    "plt.imshow(test_img_tensor.permute(1,2,0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = next(iter(dataset_loader))[0][0,:,:,:]\n",
    "#plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "output_img = model(test_img_tensor.unsqueeze(0).to(device))\n",
    "plt.imshow(output_img.squeeze().permute(1,2,0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
